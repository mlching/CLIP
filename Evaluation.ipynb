{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34178137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from textblob import TextBlob\n",
    "import csv\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "annotation_file = 'annotation.json'\n",
    "results_file = 'results.json'\n",
    "nltk.download('wordnet')\n",
    "N = type(None)\n",
    "V = np.array\n",
    "ARRAY = np.ndarray\n",
    "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
    "VS = Union[Tuple[V, ...], List[V]]\n",
    "VN = Union[V, N]\n",
    "VNS = Union[VS, N]\n",
    "T = torch.Tensor\n",
    "TS = Union[Tuple[T, ...], List[T]]\n",
    "TN = Optional[T]\n",
    "TNS = Union[Tuple[TN, ...], List[TN]]\n",
    "TSN = Optional[TS]\n",
    "TA = Union[T, ARRAY]\n",
    "\n",
    "D = torch.device\n",
    "CPU = torch.device('cpu')\n",
    "\n",
    "def get_device(device_id: int) -> D:\n",
    "    if not torch.cuda.is_available():\n",
    "        return CPU\n",
    "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
    "    return torch.device(f'cuda:{device_id}')\n",
    "\n",
    "CUDA = get_device\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: T) -> T:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) -1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    #@functools.lru_cache #FIXME\n",
    "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
    "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
    "        else:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n",
    "\n",
    "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
    "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
    "\n",
    "    model.eval()\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    tokens = None\n",
    "    scores = None\n",
    "    device = next(model.parameters()).device\n",
    "    seq_lengths = torch.ones(beam_size, device=device)\n",
    "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
    "    with torch.no_grad():\n",
    "        if embed is not None:\n",
    "            generated = embed\n",
    "        else:\n",
    "            if tokens is None:\n",
    "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                tokens = tokens.unsqueeze(0).to(device)\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "        for i in range(entry_length):\n",
    "            outputs = model.gpt(inputs_embeds=generated)\n",
    "            logits = outputs.logits\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "            logits = logits.softmax(-1).log()\n",
    "            if scores is None:\n",
    "                scores, next_tokens = logits.topk(beam_size, -1)\n",
    "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
    "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
    "                if tokens is None:\n",
    "                    tokens = next_tokens\n",
    "                else:\n",
    "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
    "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "            else:\n",
    "                logits[is_stopped] = -float(np.inf)\n",
    "                logits[is_stopped, 0] = 0\n",
    "                scores_sum = scores[:, None] + logits\n",
    "                seq_lengths[~is_stopped] += 1\n",
    "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
    "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
    "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
    "                seq_lengths = seq_lengths[next_tokens_source]\n",
    "                next_tokens = next_tokens % scores_sum.shape[1]\n",
    "                next_tokens = next_tokens.unsqueeze(1)\n",
    "                tokens = tokens[next_tokens_source]\n",
    "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "                generated = generated[next_tokens_source]\n",
    "                scores = scores_sum_average * seq_lengths\n",
    "                is_stopped = is_stopped[next_tokens_source]\n",
    "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
    "            if is_stopped.all():\n",
    "                break\n",
    "    scores = scores / seq_lengths\n",
    "    output_list = tokens.cpu().numpy()\n",
    "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
    "    order = scores.argsort(descending=True)\n",
    "    output_texts = [output_texts[i] for i in order]\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def generate2(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokens=None,\n",
    "        prompt=None,\n",
    "        embed=None,\n",
    "        entry_count=1,\n",
    "        entry_length=67,  # maximum number of words\n",
    "        top_p=0.8,\n",
    "        temperature=1.,\n",
    "        stop_token: str = '.',\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "            if embed is not None:\n",
    "                generated = embed\n",
    "            else:\n",
    "                if tokens is None:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "\n",
    "                outputs = model.gpt(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                                                    ..., :-1\n",
    "                                                    ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]\n",
    "\n",
    "is_gpu = False #@param {type:\"boolean\"}\n",
    "\n",
    "device = CUDA(0) if is_gpu else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "prefix_length = 10\n",
    "\n",
    "model = ClipCaptionModel(prefix_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82313ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation with metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fc762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model path\n",
    "model_path = \"./checkpoints/split8_gpt-017.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=CPU), strict=False)\n",
    "\n",
    "model = model.eval()\n",
    "device = CUDA(0) if is_gpu else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "use_beam_search = False #@param {type:\"boolean\"}\n",
    "\n",
    "#set image folder\n",
    "IMAGE_FOLDER = \"./testing3\"\n",
    "\n",
    "reference = []\n",
    "#set path of csv file containing ground truth\n",
    "with open(\"testing3.csv\", newline='') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for row in csvreader:\n",
    "        reference.append(row[1])\n",
    "# sort the image names with name photo[index].jpg\n",
    "def numeric_sort(file_path):\n",
    "    prefix, suffix = file_path.split(\"/\")[-1].split(\".\")[0].split(\"photo\")\n",
    "    return int(suffix)\n",
    "\n",
    "# Get list of image file paths\n",
    "image_paths = sorted(glob.glob(IMAGE_FOLDER + \"/*.jpg\"), key=numeric_sort)\n",
    "\n",
    "time_sum = 0\n",
    "i = 0\n",
    "bleu1 = []\n",
    "bleu2 = []\n",
    "bleu3 = []\n",
    "bleu4 = []\n",
    "meteor = []\n",
    "rougel = []\n",
    "cider = []\n",
    "spice = []\n",
    "subject = []\n",
    "# set path of csv file to write evaluation results\n",
    "with open('./csv/clipcap_testing3_split8_gpt-017.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File Name', 'Output', 'Time Taken', 'Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'METEOR', 'ROUGE_L', 'CIDEr', 'SPICE', 'subjectivity'])\n",
    "\n",
    "    # Loop over each image file path\n",
    "    for image_path in image_paths:\n",
    "        start_time = time.time()\n",
    "        # Open image using PIL\n",
    "        img = Image.open(image_path)\n",
    "        image = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # if type(model) is ClipCaptionE2E:\n",
    "            #     prefix_embed = model.forward_image(image)\n",
    "            # else:\n",
    "            prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "            prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "        if use_beam_search:\n",
    "            generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
    "        else:\n",
    "            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "\n",
    "\n",
    "        # Display image and output\n",
    "        end_time = time.time()\n",
    "        time_taken = round(end_time - start_time, 2)\n",
    "        time_sum += time_taken\n",
    "\n",
    "\n",
    "        display(img)\n",
    "        print(\"Reference:\", reference[i])\n",
    "        print(\"Generated caption:\", generated_text_prefix)\n",
    "        # Extract file name from the path\n",
    "        file_name = os.path.basename(image_path)\n",
    "        print(\"File name:\", file_name)\n",
    "        print(\"Time taken: {}\".format(time_taken))\n",
    "        \n",
    "        image_file = 'annotation.jpg'\n",
    "        # Create a dictionary with information about the image\n",
    "        image_info = {\n",
    "            'id': 0,\n",
    "            'file_name': image_file,\n",
    "            'width': 640,\n",
    "            'height': 480\n",
    "        }\n",
    "\n",
    "        # Create a list with information about the annotation (only one image, so the image_id is always 0)\n",
    "        annotation_info = [\n",
    "            {\n",
    "                'id': 0,\n",
    "                'image_id': 0,\n",
    "                'caption': reference[i],\n",
    "                'category_id': 0\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Create a dictionary with information about the categories\n",
    "        category_info = [\n",
    "            {\n",
    "                'id': 0,\n",
    "                'name': 'cat'\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Create a dictionary with information about the dataset\n",
    "        annotation_info = {\n",
    "            'info': {\n",
    "                'description': 'One image dataset',\n",
    "                'version': '1.0',\n",
    "                'year': 2022,\n",
    "                'contributor': 'Your Name'\n",
    "            },\n",
    "            'licenses': [],\n",
    "            'images': [image_info],\n",
    "            'annotations': annotation_info,\n",
    "            'categories': category_info\n",
    "        }\n",
    "\n",
    "\n",
    "        with open(annotation_file, 'w') as f:\n",
    "            json.dump(annotation_info, f)\n",
    "            results_info = {\"image_id\": 0, \"caption\": generated_text_prefix}\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump([results_info], f)\n",
    "\n",
    "        # Create a COCO object for the annotations and a COCO object for the predicted captions\n",
    "        coco = COCO(annotation_file)\n",
    "        coco_result = coco.loadRes(results_file)\n",
    "\n",
    "        # Create a COCOEvalCap object using the annotations and predicted captions\n",
    "        coco_eval = COCOEvalCap(coco, coco_result)\n",
    "\n",
    "        # Set the image IDs to evaluate (this can be a subset, or all images)\n",
    "        coco_eval.params['image_id'] = coco_result.getImgIds()\n",
    "\n",
    "        # Evaluate the results using the COCO evaluation metrics (this can take a few minutes)\n",
    "        coco_eval.evaluate()\n",
    "        subjectivity_score = TextBlob(generated_text_prefix).subjectivity\n",
    "        print(\"Subjectivity: \", subjectivity_score)\n",
    "        \n",
    "\n",
    "\n",
    "        # Write the image details to the CSV file\n",
    "        print(\"i: \", i)\n",
    "        writer.writerow([file_name, generated_text_prefix, time_taken, coco_eval.eval['Bleu_1'], coco_eval.eval['Bleu_2'], coco_eval.eval['Bleu_3'], coco_eval.eval['Bleu_4'], coco_eval.eval['METEOR'], coco_eval.eval['ROUGE_L'], coco_eval.eval['CIDEr'], coco_eval.eval['SPICE'], subjectivity_score])\n",
    "        \n",
    "        i += 1\n",
    "        bleu1.append(coco_eval.eval['Bleu_1'])\n",
    "        bleu2.append(coco_eval.eval['Bleu_2'])\n",
    "        bleu3.append(coco_eval.eval['Bleu_3'])\n",
    "        bleu4.append(coco_eval.eval['Bleu_4'])\n",
    "        meteor.append(coco_eval.eval['METEOR'])\n",
    "        rougel.append(coco_eval.eval['ROUGE_L'])\n",
    "        cider.append(coco_eval.eval['CIDEr'])\n",
    "        spice.append(coco_eval.eval['SPICE'])\n",
    "\n",
    "        subject.append(subjectivity_score)\n",
    "    writer.writerow([f\"{sum(bleu1)/len(image_paths):.6f}\", f\"{sum(bleu2)/len(image_paths):.6f}\", f\"{sum(bleu3)/len(image_paths):.6f}\", f\"{sum(bleu4)/len(image_paths):.6f}\", f\"{sum(meteor)/len(image_paths):.6f}\", f\"{sum(rougel)/len(image_paths):.6f}\", f\"{sum(spice)/len(image_paths):.6f}\", f\"{sum(subject)/len(image_paths):.6f}\"])\n",
    "        \n",
    "\n",
    "print(f\"Average time taken:{time_sum/len(image_paths):.6f}\")\n",
    "print(f\"Average Bleu1 score:{sum(bleu1)/len(image_paths):.6f}\")\n",
    "print(f\"Average Bleu2 score:{sum(bleu2)/len(image_paths):.6f}\")\n",
    "print(f\"Average Bleu3 score:{sum(bleu3)/len(image_paths):.6f}\")\n",
    "print(f\"Average Bleu4 score:{sum(bleu4)/len(image_paths):.6f}\")\n",
    "print(f\"Average Meteor score:{sum(meteor)/len(image_paths):.6f}\")\n",
    "print(f\"Average ROUGEL score:{sum(rougel)/len(image_paths):.6f}\")\n",
    "print(f\"Average CIDEr score:{sum(cider)/len(image_paths):.6f}\")\n",
    "print(f\"Average SPICE score:{sum(spice)/len(image_paths):.6f}\")\n",
    "print(f\"Average subjectivity score:{sum(subject)/len(image_paths):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation without ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set image folder\n",
    "IMAGE_FOLDER = \"./testing2\"\n",
    "#set model weight\n",
    "model_path = \"./checkpoints/vizcofk3_gpt-006.pt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=CPU), strict=False)\n",
    "\n",
    "model = model.eval()\n",
    "device = CUDA(0) if is_gpu else \"cpu\"\n",
    "model = model.to(device)\n",
    "# Define custom sort function that sorts file names based on numeric suffixes\n",
    "def numeric_sort(file_path):\n",
    "    prefix, suffix = file_path.split(\"/\")[-1].split(\".\")[0].split(\"photo\")\n",
    "    return int(suffix)\n",
    "\n",
    "# Get list of image file paths\n",
    "image_paths = sorted(glob.glob(IMAGE_FOLDER + \"/*.jpg\") + glob.glob(IMAGE_FOLDER + \"/*.JPG\"), key=numeric_sort)\n",
    "\n",
    "# Define custom sort function that sorts file names based on numeric suffixes\n",
    "def numeric_sort(file_path):\n",
    "    prefix, suffix = file_path.split(\"/\")[-1].split(\".\")[0].split(\"photo\")\n",
    "    return int(suffix)\n",
    "\n",
    "# Get list of image file paths\n",
    "image_paths = sorted(glob.glob(IMAGE_FOLDER + \"/*.jpg\"), key=numeric_sort)\n",
    "\n",
    "# set path of csv file to write evaluation results\n",
    "with open('./csv/CLipCap_testing2_vizcofk3_gpt-006.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File Name', 'Output', 'Time Taken'])\n",
    "\n",
    "    # Loop over each image file path\n",
    "    for image_path in image_paths:\n",
    "        start_time = time.time()\n",
    "        # Open image using PIL\n",
    "        image = io.imread(image_path)\n",
    "        pil_image = PIL.Image.fromarray(image)\n",
    "        image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # if type(model) is ClipCaptionE2E:\n",
    "            #     prefix_embed = model.forward_image(image)\n",
    "            # else:\n",
    "            prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "            prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "        if use_beam_search:\n",
    "            generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
    "        else:\n",
    "            try:\n",
    "                generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "            except TypeError:\n",
    "                continue\n",
    "\n",
    "        # Display image and output\n",
    "        end_time = time.time()\n",
    "        time_taken = round(end_time - start_time, 2)\n",
    "        time_sum += time_taken\n",
    "\n",
    "\n",
    "        display(pil_image)\n",
    "        print(\"Generated caption:\", generated_text_prefix)\n",
    "        # Extract file name from the path\n",
    "        file_name = os.path.basename(image_path)\n",
    "        print(\"File name:\", file_name)\n",
    "        print(\"Time taken: {}\".format(time_taken))\n",
    "\n",
    "        # Write the image details to the CSV file\n",
    "        writer.writerow([file_name, generated_text_prefix, time_taken])\n",
    "\n",
    "print(\"Average time taken:\", time_sum/len(image_paths))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
